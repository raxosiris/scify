{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "import en_core_sci_lg\n",
    "from scify.nlp import *\n",
    "from scify.utils.GNBR_preprocessing import get_data_and_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMLS is really big. Better to load linkers in separate cells. Sometimes crashes or similar on my Macbook Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markus/opt/anaconda3/envs/markus_nlp/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/markus/opt/anaconda3/envs/markus_nlp/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This line takes a while, because we have to download ~1GB of data\n",
    "# and load a large JSON file (the knowledge base). Be patient!\n",
    "# Thankfully it should be faster after the first time you use it, because\n",
    "# the downloads are cached.\n",
    "# NOTE: The resolve_abbreviations parameter is optional, and requires that\n",
    "# the AbbreviationDetector pipe has already been added to the pipeline. Adding\n",
    "# the AbbreviationDetector pipe and setting resolve_abbreviations to True means\n",
    "# that linking will only be performed on the long form of abbreviations.\n",
    "\n",
    "#UMLS is big and takes a while EVEN when CACHED it is downloading from tfidf_vectors from here idk why:\n",
    "#url: /ai2-s2-scispacy/data/umls/tfidf_vectors_sparse.npz\n",
    "linker_umls = EntityLinker(resolve_abbreviations=True, name=\"umls\")\n",
    "\n",
    "#linker_go = EntityLinker(resolve_abbreviations=True, name=\"go\")\n",
    "#linker_mesh = EntityLinker(resolve_abbreviations=True, name=\"mesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linkers = [EntityLinker(resolve_abbreviations=True, name=ontology) for ontology in [\"umls\", \"go\", \"mesh\"]] this might crash\n",
    "#linkers = [linker_mesh, linker_go, linker_umls]\n",
    "#[get_ontology_name(linker) for linker in linkers]\n",
    "\n",
    "#NER_models = [en_ner_bc5cdr_md, en_ner_craft_md, en_ner_bionlp13cg_md, en_ner_jnlpba_md]\n",
    "#nlps = [model.load() for model in NER_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specialized NER models trained on specialized corpus\n",
    "import en_ner_bc5cdr_md\n",
    "import en_ner_craft_md\n",
    "import en_ner_bionlp13cg_md\n",
    "import en_ner_jnlpba_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Spinal and bulbar muscular atrophy (SBMA) is an \\\n",
    "           inherited motor neuron disease caused by the expansion \\\n",
    "           of a polyglutamine tract within the androgen receptor (AR). \\\n",
    "           SBMA can be caused by this easily.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm loading models here in every cell (instead of mapping them) separately because it sometimes crashes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nlp = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_ner_craft_md\") #craft has loading issues? buggy...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp3 = en_ner_bionlp13cg_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp4 = en_ner_jnlpba_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_nlps = [nlp1, nlp2, nlp3, nlp4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlps = add_pipes_mutative(NER_nlps, linker_umls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nlp.pipeline, nlps[3].pipeline, nlps[0].meta[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be Very careful to overwrite Ents .... there's many attributes that are needed! I just attach an extension\n",
    "(but in the end, my algorithm just looks at labels_ to determine if a sentence is worthy of pattern matching!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) _token the product(NER modules x mesh/go/umls)\n",
    "2) Read the Text with all of them\n",
    "3) Extract their labels and ids + confidence scores\n",
    "4) Look at set differences -> overlap intersect?\n",
    "4) Add labels to doc @ Scispacy_lg (no NER) so it has Labels (set_extension)\n",
    "5) Assign highest confidence label to scispacy large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linker_umls.kb.cui_to_entity[\"5372017\"]\n",
    "#D016472 is a mesh ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_gene_in_texts = [\n",
    "    \"The antiarrhythmic effect of carvedilol was examined in a placebo-controlled multicenter trial , the Carvedilol Post-Infarct Survival Control in Left_Ventricular_Dysfunction -LRB- CAPRICORN -RRB- study , which enrolled 1,959 patients with reduced left ventricular systolic function after AMI , 98 % of whom were treated with an ACE inhibitor .\",\n",
    "   \"Carvedilol improves survival of patients suffering from CHF but the effects of the drug on angiotensin-converting_enzyme -LRB- ACE -RRB- activity , renin and aldosterone are not well characterized in patients receiving an ACE inhibitor .\",\n",
    "\"We therefore assessed the effects of introducing carvedilol and bisoprolol in a prospective manner on indices of oxidative stress -LSB- lipid hydroperoxides -LRB- LHP -RRB- -RSB- , endothelial damage -LSB- von_Willebrand factor -LRB- vWf -RRB- -RSB- , platelet activation -LRB- soluble P-selectin -RRB- and coagulation -LRB- fibrinogen -RRB- and their inter-relationships in stable outpatients with CHF in sinus rhythm .\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "docs = get_merged_docs_for_texts(chem_gene_in_texts, base_nlp, nlps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_doc(docs[0], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scify.nlp import construct_pattern, prep_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_pattern(prep_pattern(\"kinase|compound|START_ENTITY RAFTK|nsubj|kinase RAFTK|dep|is is|nsubj|kinase kinase|nmod|activation activation|nmod|END_ENTITY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"\"\"In this study , we report that the related adhesion focal tyrosine kinase RAFTK , is an upstream kinase in beta1 integrin mediated activation of Akt.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_doc(base_nlp(sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_non_zero support for one of the 30 themes only\n",
    "#dependency paths + support\n",
    "#Maybe merge dependency paths\n",
    "#distributions\n",
    "\n",
    "#it's very big but looks like this\n",
    "distributions_kv_example = {\"kinases|compound|START_ENTITY participate|nsubj|kinases participate|nmod|END_ENTITY\": {'A+': 2.0,\n",
    "  'A+.ind': 0.0,\n",
    "  'A-': 0.0,\n",
    "  'A-.ind': 0.0,\n",
    "  'B': 0.0,\n",
    "  'B.ind': 0.0,\n",
    "  'E+': 0.0,\n",
    "  'E+.ind': 0.0,\n",
    "  'E-': 0.0,\n",
    "  'E-.ind': 0.0,\n",
    "  'E': 0.0,\n",
    "  'E.ind': 0.0,\n",
    "  'N': 0.0,\n",
    "  'N.ind': 0.0,\n",
    "  'O': 2.0,\n",
    "  'O.ind': 0.0,\n",
    "  'K': 0.0,\n",
    "  'K.ind': 0.0,\n",
    "  'Z': 0.0,\n",
    "  'Z.ind': 0.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scify.nlp import rgetattr, rsetattr\n",
    "linker_umls.kb.cui_to_entity[\"C1413931\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[annotation[\"label\"] \n",
    " for ent in docs[0].ents \n",
    " for annotation in ent._.annotated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc for doc in docs if doc_has_entity_labels(doc,  [[\"CHEMICAL\", \"SIMPLE_CHEMICAL\"], [\"GENE\", \"GENE_OR_GENE_PRODUCT\"]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #from spacy.tokens import DocBin\n",
    "import pickle\n",
    "doc_data = pickle.dumps(docs) #only pickle lists of docs so it only includes their shared vocabulary\n",
    "with open('../data/pubmed_annotated.pickle', 'wb') as f:\n",
    "    pickle.dump(doc_data, f)\n",
    "print(len(doc_data)) #that's a lot for 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_data2 = pickle.dumps(docs[1:]) #only pickle lists of docs so it only includes their shared vocabulary\n",
    "with open('../data/pubmed_annotated.pickle', 'wb') as f:\n",
    "    pickle.dump(doc_data, f)\n",
    "print(len(doc_data2)) #ok but it's only the vocab and scales well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, distributions = get_data_and_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [dat[\"sent\"] for dat in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sents)\n",
    "sent_sample = sents[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_merged_docs_for_texts(sent_sample, base_nlp, nlps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_sents = [doc for doc in docs if doc_has_entity_labels(doc,  [[\"CHEMICAL\", \"SIMPLE_CHEMICAL\"], [\"GENE\", \"GENE_OR_GENE_PRODUCT\"]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our NER modules work. Theoretically every sentence in the data should have at least one chemical and one gene entity in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that's 40% coverage....that's kinda bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wanted_sents), wanted_sents[200]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [d[\"dep\"] for d in data[:500]]\n",
    "\n",
    "constructed = [construct_pattern(pattern) for pattern in patterns]\n",
    "\n",
    "len([con for con in constructed if con != None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can live with that ratio 430/500 (maybe errors not evenly distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"../data/pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pubmed_parser import parse_medline_xml\n",
    "pubmed_abstracts = parse_medline_xml(\"../data/pubmed/pubmed20n1015.xml\")\n",
    "abstr = [article[\"abstract\"] for article in pubmed_abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "#already in pickle\n",
    "#docs_pubmed = get_merged_docs_for_texts(abstr, base_nlp, nlps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy #from spacy.tokens import DocBin\n",
    "import pickle\n",
    "\n",
    "def w():\n",
    "    doc_data = pickle.dumps(docs_pubmed) #only pickle lists of docs so it only includes their shared vocabulary\n",
    "    with open('../data/pubmed_annotated.pickle', 'wb') as f:\n",
    "        pickle.dump(doc_data, f)\n",
    "    print(len(doc_data)) #that's a lot for 3 sentencesv\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sent.ents for sent in dd.sents]\n",
    "from typeguard import typechecked\n",
    "\n",
    "from scify.nlp import *\n",
    "\n",
    "[doc_has_entity_labels(sent, [[\"EE\", \"EE\"], [\"rr\", \"rr\"]]) for sent in dd.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_abstr = [doc for doc in docs_pubmed if doc_has_entity_labels(doc,  [[\"CHEMICAL\", \"SIMPLE_CHEMICAL\"], [\"GENE\", \"GENE_OR_GENE_PRODUCT\"]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len([sent for abstract in wanted_abstr for sent in abstract.sents ]), len([sent for abstract in wanted_abstr for sent in abstract.sents if doc_has_entity_labels(sent,  [[\"CHEMICAL\", \"SIMPLE_CHEMICAL\"], [\"GENE\", \"GENE_OR_GENE_PRODUCT\"]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_sents =[sent for abstract in wanted_abstr for sent in abstract.sents if doc_has_entity_labels(sent,  [[\"CHEMICAL\", \"SIMPLE_CHEMICAL\"], [\"GENE\", \"GENE_OR_GENE_PRODUCT\"]])]\n",
    "pubmed_sents[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_doc(pubmed_sents[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_noun_chunks(pubmed_sents[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_dep_path(ent_a, ent_b):\n",
    "    #networkx\n",
    "\n",
    "def pattern_from_shortest_dep_path():\n",
    "    5\n",
    "    \n",
    "def extract_pattern_between_entities(doc, ent_a, ent_b):\n",
    "    4\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scify.networks import get_edges\n",
    "import networkx as nx\n",
    "get_edges(pubmed_sents[22])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docs_pubmed[22]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "[* product([\"Yolo\", \"Fredo\", \"checko\"], [\"fatty\", \"fagg\"])]\n",
    "tokens_lookup = {tok.text:tok for tok in doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#token -> char overlap in merge_docs\n",
    "#document level matching %\n",
    "#sentence level matching %\n",
    "#performance?\n",
    "\n",
    "#get_shortest_path between ents\n",
    "#find pattern matches\n",
    "#dependency pattern + pattern matcher on GNBR --> relation extraction precision?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:25]:\n",
    "    print(token, token.i, token.ent_iob)\n",
    "print([noun for noun in doc[:25].noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "def semantic_spans_from_doc(doc):\n",
    "    \"\"\"returns a list of all spans eg. [token, token, entitiy (spanning more tokens), token]\n",
    "    This is so you can iterate through it under one span API\n",
    "    \n",
    "    iobs\n",
    "    -3 begins\n",
    "    -2 outside\n",
    "    -1 inside\n",
    "    -0 no tag of\n",
    "    ...entity\n",
    "    \"\"\"\n",
    "    \"3 1 1\"\n",
    "    \"3 2 3 1\"\n",
    "    \"2 3 1\"\n",
    "    spans = []\n",
    "    current_span = []\n",
    "    for idx, token in enumerate(doc):\n",
    "        iob = token.ent_iob\n",
    "        if (iob != 1):\n",
    "            spans.append(Span)\n",
    "            current_span.append()\n",
    "        \n",
    "        \n",
    "    return spans, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1 ,2 ,3]\n",
    "\n",
    "a.pop(0), a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "copy.deepcopy(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_idxs = [ent.start for ent in doc.ents]\n",
    "non_ent_idxs = [token.i for token in doc if (token.ent_iob == 2 or token.ent_iob == 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort(ent_idxs + non_ent_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spans =[]\n",
    "ent_idxs = [ent.start for ent in doc.ents]\n",
    "ents = doc.ents.copy()\n",
    "for idx, token in enumerate(doc):\n",
    "    if idx in ents_idxs:\n",
    "        spans.append(doc[idx])\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent.start for ent in doc.ents],[token.i for token in doc if (token.ent_iob == 2 or token.ent_iob == 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.i for token in doc[:25] if token.ent_iob == 3],[token.i for token in doc[:25] if token.ent_iob != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniform \n",
    "#lowest_common_ancestor span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid pattern\n",
    "construct_pattern('|appos|START_ENTITY sensitivity|nmod|END_ENTITY') or []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invalid pattern\n",
    "construct_pattern('sensitivity|nmod|END_ENTITY sensitivity|appos|START_ENTITY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scify.networks import get_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "1) isEntity in Sentence?\n",
    "2) Shortest Path -> SP\n",
    "3) compare with theme in GNBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
